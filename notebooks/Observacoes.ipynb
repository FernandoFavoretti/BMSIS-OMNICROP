{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns\n",
    "\n",
    ">***Irradiance*** = Irradiance is the radiant flux (power) received by a surface per unit area.The SI unit of irradiance is the watt per square metre (W/m2).\n",
    "\n",
    ">***Specific humidity*** = Specific humidity is a ratio of the water vapor content of the mixture to the total air content on a mass basis. (%)\n",
    "\n",
    ">***Soil Value*** =  comparative assessment of soil quality (1-100)\n",
    "\n",
    ">***Soil temperature*** = celsius temperature of the soil\n",
    "\n",
    ">***External Temperature***\n",
    "\n",
    "> ***Ion uptake*** = ions absorved\n",
    "\n",
    "> ***Daily Growth rate***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "Dúvidas/Obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nesse primeiro teste simples eu queria apenas testar os dados, os separei em arquivos e testei alguns métodos apenas para ver o comportamento. Comecei testando com a abordagem de SVR pois a mesma funciona bem com poucos dados, utilizei também a técnica de grid searchCV para encontrar os melhores hiper parametros do método, testando com diferentes kernels etc.\n",
    "\n",
    ">Como esperado os resultados não foram nada agradáveis, mas foram úteis para eu poder começar pensar \"além do básico\"\n",
    "\n",
    "\n",
    ">***O Básico***\n",
    "\n",
    ">Agora que sei um pouco mais sobre o comportamento dos dados podemos começar com as análises padrões\n",
    "\n",
    ">1. Univariate analysis (Estudando e entendendo o dado) : Média, std, IQR, Ranges etc para entendermos melhor a distribuição dos dados\n",
    "\n",
    ">2. Bi variate Analysis: Procurar relações entre os dados, tentar diminuir o numero de colunas com por exemplo PCA, e estudar algumas visualizações como a distribuição dos dados, o scatter plot etc\n",
    "\n",
    ">3. Tratar valores missing, porém a príncipio não se aplica aqui\n",
    "\n",
    ">4. Observar a existência de outliers, e se existirem decidir como serão tratados\n",
    "\n",
    ">5. Transformar variáveis, mudar a distribuição aplicando alguma função em cima de alguma coluna para padronizar a amostra, por exemplo log ou normalização\n",
    "\n",
    "Esses são os passos básicos que irei seguir, entretando acredito que isso não seja suficiente então pensei em algumas dúvidas (fora o stack básico)\n",
    "\n",
    "Para Discutir:\n",
    "\n",
    ">1. Achei a quantidade de dados um pouco baixa, posso estar errado claro, mas talvez isso seja um problema?\n",
    ">>a. Podemos juntar os dados? juntando todas as tabelas e criando novas colunas para identificar a que tipo de cultura uma linha pertence.Ou o comportamento de cada cultura é extremamente individual a ponto de não poderem ser analisadas juntas?\n",
    "\n",
    ">>b. Podemos usar técnicas para criar mais dados sintéticos seguindo padrões encontrados nos dados atuais, se a téćnica acima não puder ser usada\n",
    "\n",
    "\n",
    ">2. Uma alternativa para melhorar a assertividade do modelo pode ser criar ranges em algumas colunas ao invés de valores absolutos, ficilitaria a vida do algoritmo transformando um problema de regressão em um problema de classificação, porém ficaria mais \"generalizado\", o qu o sr. acha? (Mas isso apenas se as análises iniciais não mostrarem resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
